# -*- coding: utf-8 -*-
"""가정교육 기준서 비교 연구

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OjoZseNrZNiW28gAAtR8N7eSW_REzuOq

#한글 폰트설치
"""

# 필수 라이브러리 설치
!pip install konlpy wordcloud matplotlib

# 한글 폰트 설정 (NanumGothic)
!apt-get update -qq
!apt-get install fonts-nanum -qq

# 폰트 설정
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from wordcloud import WordCloud

# 폰트 설정 함수
def set_korean_font():
    # 폰트 경로
    font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'
    # fm._rebuild()  <- Remove or comment out this line.
    # Use the following to clear the font cache
    fm.fontManager.addfont(font_path)
    fm._load_fontmanager(try_read_cache=False) # This forces a rebuild without the internal method

    plt.rc('font', family='NanumGothic')
    plt.rcParams['axes.unicode_minus'] = False  # 마이너스 깨짐 방지
    print("한글 폰트 설정 완료:", font_path)
    return font_path

# 한글 폰트 설정 실행
font_path = set_korean_font()

# 기타 라이브러리 Import
import re
from konlpy.tag import Komoran
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# 테스트용 그래프
plt.figure(figsize=(5, 3))
plt.title("한글 폰트 테스트")
plt.plot([0, 1, 2], [0, 1, 4], label="테스트 라인")
plt.legend()
plt.show()

"""#데이터전처리"""

# 파일 경로 및 데이터 로드
file_paths = {
    "2015년 데이터": '/content/2015 중학교 기가 교과서 5종 생활자원 전체.txt',
    "2022년 데이터": '/content/20022 중학교 기가 교과서 5종 생활 자원 전체.txt',
}

category_files = [
    '/content/1. 빈곤퇴치.txt',
    '/content/2. 기아종식.txt',
    '/content/3. 건강과 웰빙중심.txt',
    '/content/4. 양질의 교육 보장.txt',
    '/content/5. 성평등 달성.txt',
    '/content/6. 깨끗한 물과 위생 보장.txt',
    '/content/7. 모두를 위한 에너지 보장.txt',
    '/content/8. 경제성장과 양질의 일자리.txt',
    '/content/9. 사회기반 시설 산업화 및 혁신.txt',
    '/content/10. 불평등 감소.txt',
    '/content/11. 지속가능한 도시와 주거지.txt',
    '/content/12. 지속가능한 소비와 생산.txt',
    '/content/13. 기후변화 대응.txt',
    '/content/14. 해양생태계 보존.txt',
    '/content/15. 육상생태계 보호.txt',
    '/content/16. 평화, 정의, 포용적인 제도.txt',
    '/content/17. 글로벌 파트너십.txt',
]

# 데이터 로드 함수
def load_texts(file_paths):
    data = {}
    for key, path in file_paths.items():
        with open(path, 'r', encoding='utf-8') as f:
            data[key] = f.read()
    return data

# 카테고리 로드 함수
def load_categories(file_paths):
    categories = []
    for path in file_paths:
        with open(path, 'r', encoding='utf-8') as f:
            categories.append(f.read())
    return categories

# 데이터 로드
text_data = load_texts(file_paths)
categories = load_categories(category_files)

# 데이터 확인
for key, content in text_data.items():
    print(f"{key} 일부:\n", content[:50])

print("category1 데이터 일부 (1부터 17까지):")
for i, category in enumerate(categories, start=1):
    print(f"문서 {i}: {category[:50]}")

import re
from konlpy.tag import Komoran

# Komoran 형태소 분석기 초기화
komoran = Komoran()

# 불용어 리스트를 공백으로 설정 (필요시 추가 가능)
stopwords = ['소비자']

def preprocess_nouns(text, stopwords=[]):
    """
    텍스트에서 한글 명사만 추출하고, 공백 및 줄바꿈 제거, 불용어 처리.
    """
    try:
        # 특수문자 및 공백 제거
        text = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣]', ' ', text)  # 한글 이외 제거 후 공백으로 대체
        text = re.sub(r'\s+', ' ', text).strip()  # 연속 공백 제거 및 양끝 공백 제거

        # 명사 추출
        nouns = komoran.nouns(text)

        # 불용어 제거
        filtered_nouns = [noun for noun in nouns if noun not in stopwords]

        # 결과를 공백으로 연결
        return ' '.join(filtered_nouns)
    except Exception as e:
        print(f"전처리 중 오류 발생: {e}")
        return ""

# 텍스트 전처리
preprocessed_texts = {key: preprocess_nouns(content, stopwords) for key, content in text_data.items()}
preprocessed_categories = [preprocess_nouns(category, stopwords) for category in categories]

# 결과 확인
print("2015년 데이터 전처리 결과:\n", preprocessed_texts["2015년 데이터"][:100])
print("2022년 데이터 전처리 결과:\n", preprocessed_texts["2022년 데이터"][:100])
print("카테고리 데이터 전처리 결과 (1부터 17까지):")
for i, category in enumerate(preprocessed_categories, start=1):
    print(f"문서 {i} 전처리 결과: {category[:100]}")

# 4. 전처리된 텍스트들을 공백 기준으로 토큰화
tokenized_texts = {key: text.split() for key, text in preprocessed_texts.items()}
tokenized_categories = [category.split() for category in preprocessed_categories]

# 5. 결과 확인
print("2015년 데이터 전처리 및 토큰화 결과:\n", tokenized_texts["2015년 데이터"])
print("\n2022년 데이터 전처리 및 토큰화 결과:\n", tokenized_texts["2022년 데이터"])
print("\n카테고리 데이터 토큰화 결과:")
for i, tokens in enumerate(tokenized_categories, start=1):
    print(f"문서 {i} 토큰화 결과: {tokens}")

from collections import Counter
# 4. 단어 빈도 계산 함수 정의
def calculate_top_frequencies(text, top_n=10):
    """
    텍스트에서 단어 빈도를 계산하고 상위 top_n 단어를 반환.
    """
    words = text.split()  # 공백 기준으로 단어 분리
    word_counts = Counter(words)  # 단어 빈도 계산
    return word_counts.most_common(top_n)  # 상위 top_n 단어 반환

# 5. 각 파일별 상위 빈도 추출
top_frequencies_texts = {
    key: calculate_top_frequencies(content, top_n=10) for key, content in preprocessed_texts.items()
}

top_frequencies_categories = [
    calculate_top_frequencies(category, top_n=10) for category in preprocessed_categories
]

# 6. 결과 확인
print("2015년 데이터 상위 빈도 결과:\n", top_frequencies_texts["2015년 데이터"])
print("\n2022년 데이터 상위 빈도 결과:\n", top_frequencies_texts["2022년 데이터"])

print("\n카테고리 데이터 상위 빈도 결과:")
for i, top_frequencies in enumerate(top_frequencies_categories, start=1):
    print(f"문서 {i} 상위 빈도 결과: {top_frequencies}")

import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer

# TfidfVectorizer가 이미 토큰화된 데이터를 처리할 수 있도록 analyzer를 직접 설정합니다.
vectorizer = TfidfVectorizer(analyzer=lambda x: x)
tfidf_matrix = vectorizer.fit_transform(tokenized_categories)
feature_names = vectorizer.get_feature_names_out()

# 각 기준서(문서)별 상위 10개 단어와 해당 TF-IDF 점수를 추출
top_words_per_category = []
for i in range(tfidf_matrix.shape[0]):
    # 각 문서의 TF-IDF 벡터를 1차원 배열로 추출
    row = tfidf_matrix.getrow(i).toarray().flatten()
    # TF-IDF 값이 큰 순서대로 인덱스를 정렬
    sorted_indices = np.argsort(row)[::-1]
    top10 = []
    for idx in sorted_indices:
        if row[idx] <= 0:  # TF-IDF 점수가 0 이하이면 중단
            break
        top10.append((feature_names[idx], row[idx]))
        if len(top10) == 10:
            break
    top_words_per_category.append(top10)
    print(f"문서 {i+1} 상위 10개 단어:", top10)

# 정확히 '물'이라는 단어만 몇 개 나오는지 확인
count_water_2015 = preprocessed_texts["2015년 데이터"].split().count("시간")
count_water_2022 = preprocessed_texts["2022년 데이터"].split().count("물")

print("2015년 데이터에서 '물' 단어 개수:", count_water_2015)
print("2022년 데이터에서 '물' 단어 개수:", count_water_2022)

counter_2015 = Counter(tokenized_texts["2015년 데이터"])
counter_2022 = Counter(tokenized_texts["2022년 데이터"])

print("기준서별 상위 10개 단어의 2015년, 2022년 등장 빈도:")
for idx, top_words in enumerate(top_words_per_category, start=1):
    print(f"\n기준서 {idx}:")
    for word, tfidf_score in top_words:
        count_2015 = counter_2015.get(word, 0)
        count_2022 = counter_2022.get(word, 0)
        print(f"  단어 '{word}': 2015년 = {count_2015}, 2022년 = {count_2022}")

import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import collections
from tqdm import tqdm

# ----------------------------------------
# 전제: text_data, categories, stopwords, preprocess_nouns,
#       top_words_per_category가 이미 정의되어 있음.
# ----------------------------------------

# ====================================================
# Step 1. 연도별 텍스트 전처리 및 토큰 카운터 생성
# ====================================================
year_token_counts = {}  # {year_label: Counter(tokens)}

for year_label, year_text in tqdm(text_data.items(), desc="연도별 전처리", unit="년"):
    # preprocess_nouns 함수는 한글 명사만 남기도록 전처리하는 함수입니다.
    preprocessed_text = preprocess_nouns(year_text, stopwords=stopwords)
    # 공백 기준 토큰화
    tokens = preprocessed_text.split()
    token_counter = collections.Counter(tokens)
    year_token_counts[year_label] = token_counter
    print(f"{year_label} - Total tokens: {len(tokens)}, Unique tokens: {len(token_counter)}")

# ====================================================
# Step 2. 각 연도별 기준서 점수 및 단어별 등장 횟수 계산
# ====================================================
results = []               # 각 기준서별 전체 점수를 저장할 리스트
detailed_word_counts = []  # 단어별 등장 횟수 및 기여 점수 상세 정보를 저장할 리스트

for year_label, token_counter in tqdm(year_token_counts.items(), desc="연도별 처리", unit="년"):
    print(f"\n===== {year_label} =====")
    for cat_idx, top_words in tqdm(enumerate(top_words_per_category, start=1),
                                   total=len(top_words_per_category),
                                   desc="기준서 처리", unit="문서", leave=False):
        cat_score = 0.0
        print(f"\n[기준서 {cat_idx}]")
        for word, weight in top_words:
            # 미리 계산된 token_counter에서 단어의 빈도 조회 (없으면 0)
            count = token_counter.get(word, 0)
            contribution = count * weight
            cat_score += contribution
            print(f"  단어 '{word}': Count = {count}, TF-IDF = {weight:.3f}, Contribution = {contribution:.3f}")
            detailed_word_counts.append({
                "Year": year_label,
                "Category": f"기준서 {cat_idx}",
                "Word": word,
                "TF-IDF": weight,
                "Count": count,
                "Contribution": contribution
            })
        print(f">> 기준서 {cat_idx} Final Score: {cat_score:.3f}")
        results.append({
            "Year": year_label,
            "Category": f"기준서 {cat_idx}",
            "Score": cat_score
        })

# ====================================================
# Step 3. 결과 DataFrame 생성 및 시각화
# ====================================================
# 전체 기준서 점수 DataFrame
df_results = pd.DataFrame(results)
print("\n===== Computed Category Scores =====")
print(df_results)

# 단어별 상세 등장 횟수 및 기여 점수 DataFrame
df_detailed = pd.DataFrame(detailed_word_counts)
print("\n===== Detailed Word Counts and Contributions =====")
print(df_detailed)

# 바 차트를 이용한 연도별, 기준서별 전체 점수 비교
plt.figure(figsize=(10, 6))
sns.barplot(x="Category", y="Score", hue="Year", data=df_results)
plt.xticks(rotation=45)
plt.title("연도별 기준서 점수 비교")
plt.tight_layout()
plt.show()

# 각 연도별 기준서 점수 요약 통계 (평균, 최소, 최대, 중앙값)
summary_stats = df_results.groupby("Year")["Score"].agg(["mean", "min", "max", "median"]).reset_index()
print("\n===== Summary Statistics by Year =====")
print(summary_stats)

"""#딥러닝방법론"""

!pip install transformers sentencepiece

import re

# 데이터 전처리 및 문장 분리 함수
def split_into_sentences(text, min_length=5):
    """
    텍스트 전처리 및 문장 분리:
    - 한글, 기본 구두점(마침표, 물음표, 느낌표)만 유지
    - 줄바꿈 -> 공백으로 대체
    - 공백 3개 이상 기준으로 분리
    - 구두점 기준으로 추가 분리
    - 최소 길이 조건으로 필터링
    """
    try:
        # 한글, 마침표, 물음표, 느낌표만 유지하고 나머지 제거
        text = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣.!?]', ' ', text)
        # 연속된 공백을 하나로 줄이기
        text = re.sub(r'\s+', ' ', text).strip()
        # 공백 3개 이상 기준으로 분리
        sentences = re.split(r'\s{3,}', text)
        # 각 문장별로 구두점 기준 추가 분리
        final_sentences = []
        for sentence in sentences:
            split_sentences = re.split(r'(?<=[.!?])\s+', sentence)
            final_sentences.extend(split_sentences)
        # 최소 길이 조건에 따라 필터링
        filtered_sentences = [s.strip() for s in final_sentences if len(s.strip()) >= min_length]
        return filtered_sentences
    except Exception as e:
        print(f"문장 전처리 및 분리 중 오류 발생: {e}")
        return []

# 2015년, 2022년 데이터를 문장 단위로 분할
sentences_2015 = split_into_sentences(text_data["2015년 데이터"])
sentences_2022 = split_into_sentences(text_data["2022년 데이터"])

# 카테고리 데이터를 문장 단위로 분할
category_sentences = [split_into_sentences(category) for category in categories]

# 문장 수 계산 및 출력
print(f"2015년 데이터 문장 수: {len(sentences_2015)}")
print(f"2022년 데이터 문장 수: {len(sentences_2022)}")

print("\n카테고리별 문장 수:")
for i, category in enumerate(category_sentences, start=1):
    print(f"문서 {i}: {len(category)} 문장")

from sentence_transformers import SentenceTransformer, util
import torch

# SBERT 모델 초기화
model = SentenceTransformer('sentence-transformers/xlm-r-large-en-ko-nli-ststb')  # 한국어 SBERT

# 2015년, 2022년 문장 임베딩 실행
print("2015년 문장 임베딩 생성 중...")
embeddings_2015 = model.encode(sentences_2015, convert_to_tensor=True)

print("2022년 문장 임베딩 생성 중...")
embeddings_2022 = model.encode(sentences_2022, convert_to_tensor=True)

# 문서 1~17의 문장 임베딩 실행
category_embeddings = []
for i, category_sentences_group in enumerate(category_sentences, start=1):
    print(f"문서 {i} 문장 임베딩 생성 중...")
    category_embeddings.append(model.encode(category_sentences_group, convert_to_tensor=True))

# 임베딩 크기 확인
print("\n임베딩 크기:")
print("2015년 문장 임베딩 크기:", embeddings_2015.size())
print("2022년 문장 임베딩 크기:", embeddings_2022.size())
for i, embedding in enumerate(category_embeddings, start=1):
    print(f"문서 {i} 문장 임베딩 크기:", embedding.size())

# 문서 1~17 유사도 계산 및 저장
similarities_by_document = {}

for i, doc_embedding in enumerate(category_embeddings, start=1):
    print(f"\n문서 {i} 유사도 계산 중...")

    # 문서와 2015년도 데이터 간 유사도 계산
    similarities_2015 = util.pytorch_cos_sim(doc_embedding, embeddings_2015)

    # 문서와 2022년도 데이터 간 유사도 계산
    similarities_2022 = util.pytorch_cos_sim(doc_embedding, embeddings_2022)

    # 유사도 결과 저장
    similarities_by_document[f"문서 {i}"] = {
        "2015": similarities_2015,
        "2022": similarities_2022
    }

# 문서별 유사도 결과 출력
for doc, data in similarities_by_document.items():
    print(f"\n{doc} 유사도 결과:")

    # 2015년도 유사도 분석
    avg_similarity_2015 = torch.mean(data["2015"])
    max_similarity_2015 = torch.max(data["2015"])
    print(f"2015년도와의 평균 유사도: {avg_similarity_2015:.4f}, 최대 유사도: {max_similarity_2015:.4f}")

    # 2022년도 유사도 분석
    avg_similarity_2022 = torch.mean(data["2022"])
    max_similarity_2022 = torch.max(data["2022"])
    print(f"2022년도와의 평균 유사도: {avg_similarity_2022:.4f}, 최대 유사도: {max_similarity_2022:.4f}")

import matplotlib.pyplot as plt

# 문서별 유사도 히스토그램
for doc, data in similarities_by_document.items():
    plt.figure(figsize=(12, 6))
    plt.hist(data["2015"].flatten().cpu().numpy(), bins=50, alpha=0.7, label="2015년 유사도")
    plt.hist(data["2022"].flatten().cpu().numpy(), bins=50, alpha=0.7, label="2022년 유사도")
    plt.title(f"{doc} 유사도 분포 (2015년 vs 2022년)", fontsize=14, fontweight="bold")
    plt.xlabel("유사도", fontsize=12)
    plt.ylabel("빈도", fontsize=12)
    plt.legend(fontsize=12)
    plt.grid(axis="y", linestyle="--", alpha=0.7)
    plt.tight_layout()
    plt.show()

"""#jhgan/ko-sroberta-multitask"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import collections
from sentence_transformers import SentenceTransformer, util

# --------------------------
# 1. 모델 로드 (한국어 최적 모델)
# --------------------------
model = SentenceTransformer("sentence-transformers/xlm-r-large-en-ko-nli-ststb")

# --------------------------
# 2. 미리 분할된 문장 리스트가 있다고 가정
#    - sentences_2015: 2015년도 문장 리스트
#    - sentences_2022: 2022년도 문장 리스트
#    - category_sentences: 기준서(문서 1~17)별 문장 리스트 (리스트 안에 17개의 리스트)
# --------------------------

# 2015, 2022년도 문장의 임베딩을 미리 계산 (한번에 계산)
embeddings_2015 = model.encode(sentences_2015, convert_to_tensor=True)
embeddings_2022 = model.encode(sentences_2022, convert_to_tensor=True)

# --------------------------
# 3. 각 기준서 문서(문서 1~17)와 각 연도(2015, 2022) 문장 간의 유사도 계산 및 분석
# --------------------------
for cat_idx, cat_sent_list in enumerate(category_sentences, start=1):
    print(f"\n===== 기준서 문서 {cat_idx} =====")

    # 기준서 문장의 임베딩 계산
    cat_embeddings = model.encode(cat_sent_list, convert_to_tensor=True)

    # 연도별로 처리 (2015년과 2022년)
    for year_label, year_embeddings, year_sentences in zip(
        ["2015년", "2022년"],
        [embeddings_2015, embeddings_2022],
        [sentences_2015, sentences_2022]
    ):
        print(f"\n--- {year_label} ---")

        # 기준서 문장과 해당 연도 문장 사이의 코사인 유사도 행렬 계산
        # sim_matrix.shape = (len(cat_sent_list), len(year_sentences))
        sim_matrix = util.cos_sim(cat_embeddings, year_embeddings)

        # 전체 유사도 값을 1차원 배열로 변환
        sim_scores = sim_matrix.cpu().numpy().flatten()

        # 유사도 통계 계산
        mean_sim = np.mean(sim_scores)
        min_sim = np.min(sim_scores)
        max_sim = np.max(sim_scores)
        # 최빈값 계산: 소수점 둘째자리로 반올림하여 빈도수 집계
        rounded_scores = np.round(sim_scores, 2)
        counter = collections.Counter(rounded_scores)
        mode_val, mode_count = counter.most_common(1)[0]

        print(f"유사도 통계 - 평균: {mean_sim:.3f}, 최소: {min_sim:.3f}, 최대: {max_sim:.3f}, 최빈값: {mode_val} (빈도: {mode_count})")

        # --------------------------
        # 3-1. 박스플롯으로 유사도 분포 시각화
        # --------------------------
        plt.figure(figsize=(6,4))
        sns.boxplot(x=sim_scores)
        plt.title(f"기준서 {cat_idx} - {year_label} 유사도 박스플롯")
        plt.xlabel("코사인 유사도")
        plt.show()

        # --------------------------
        # 3-2. 히스토그램으로 구간별 문장 수 시각화
        # --------------------------
        plt.figure(figsize=(6,4))
        bins = np.linspace(0, 1, 21)  # 0~1 범위를 20 구간으로 나눔
        plt.hist(sim_scores, bins=bins, edgecolor='black')
        plt.title(f"기준서 {cat_idx} - {year_label} 유사도 분포")
        plt.xlabel("코사인 유사도")
        plt.ylabel("문장 개수")
        plt.show()

        # --------------------------
        # 3-3. 가장 유사한 문장 쌍 찾기
        # --------------------------
        max_idx = np.argmax(sim_scores)
        # 2차원 행렬 인덱스로 변환: 행(기준서 문장 인덱스), 열(연도 문장 인덱스)
        num_year = sim_matrix.shape[1]
        cat_sentence_index = max_idx // num_year
        year_sentence_index = max_idx % num_year
        highest_sim = sim_scores[max_idx]

        print(f"\n가장 유사한 문장 쌍 (유사도 {highest_sim:.3f}):")
        print(" [기준서 문장]:", cat_sent_list[cat_sentence_index])
        print(" [연도 문장]:", year_sentences[year_sentence_index])

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import collections
import re
from sentence_transformers import SentenceTransformer, util

# ====================================
# 1. 문장 분리 함수 (전처리)
# ====================================
def split_into_sentences(text, min_length=5):
    """
    텍스트 전처리 및 문장 분리:
    - 한글, 마침표, 물음표, 느낌표만 남김
    - 줄바꿈은 공백으로 대체 후, 연속 공백 줄이기
    - 공백 3개 이상 기준 분리 후, 구두점 기준 추가 분리
    - 최소 길이 조건 필터링
    """
    try:
        text = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣.!?]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        sentences = re.split(r'\s{3,}', text)
        final_sentences = []
        for sentence in sentences:
            split_sentences = re.split(r'(?<=[.!?])\s+', sentence)
            final_sentences.extend(split_sentences)
        filtered_sentences = [s.strip() for s in final_sentences if len(s.strip()) >= min_length]
        return filtered_sentences
    except Exception as e:
        print(f"문장 분리 중 오류: {e}")
        return []

# 이미 준비된 text_data, categories 변수가 있다고 가정합니다.
# 예시:
# text_data = {"2015년 데이터": "…", "2022년 데이터": "…"}
# categories = [기준서1, 기준서2, ..., 기준서17]

# 2015년, 2022년 데이터를 문장 단위로 분리
sentences_2015 = split_into_sentences(text_data["2015년 데이터"])
sentences_2022 = split_into_sentences(text_data["2022년 데이터"])

# 각 기준서(정부 지침서)도 문장 단위로 분리 (17개 문서)
category_sentences = [split_into_sentences(category) for category in categories]

# ====================================
# 2. 한국어에 적합한 SentenceTransformer 모델 로드
# ====================================
# 공개된 한국어 모델로 교체 (jhgan/ko-sroberta-multitask)
model = SentenceTransformer("sentence-transformers/xlm-r-large-en-ko-nli-ststb")

# 2015년과 2022년 문장들의 임베딩은 미리 계산 (속도 향상)
embeddings_2015 = model.encode(sentences_2015, convert_to_tensor=True)
embeddings_2022 = model.encode(sentences_2022, convert_to_tensor=True)

# ====================================
# 3. 기준서별(문서 1~17) 유사도 비교 및 시각화
# ====================================
all_summary = []  # 전체 요약 통계 저장

for cat_idx, cat_sent_list in enumerate(category_sentences, start=1):
    print(f"\n===== 기준서 문서 {cat_idx} =====")

    # 기준서 문장 임베딩 계산
    cat_embeddings = model.encode(cat_sent_list, convert_to_tensor=True)

    # 딕셔너리로 연도별 유사도 저장
    sim_scores_dict = {}
    summary_stats = {}
    best_sentence_pair = {}

    # 2015년과 2022년 각각 처리
    for year_label, year_embeddings, year_sentences in zip(
        ["2015년", "2022년"],
        [embeddings_2015, embeddings_2022],
        [sentences_2015, sentences_2022]
    ):
        # 기준서와 연도 문장 간 코사인 유사도 계산
        sim_matrix = util.cos_sim(cat_embeddings, year_embeddings)
        sim_scores = sim_matrix.cpu().numpy().flatten()
        sim_scores_dict[year_label] = sim_scores

        # 요약 통계 계산
        mean_sim = np.mean(sim_scores)
        min_sim = np.min(sim_scores)
        max_sim = np.max(sim_scores)
        rounded_scores = np.round(sim_scores, 2)
        counter = collections.Counter(rounded_scores)
        mode_val, mode_count = counter.most_common(1)[0]

        summary_stats[year_label] = {
            'Mean': mean_sim,
            'Min': min_sim,
            'Max': max_sim,
            'Mode': mode_val,
            'Mode Count': mode_count
        }

        # 가장 유사한 문장 쌍 찾기
        max_idx = np.argmax(sim_scores)
        num_year = sim_matrix.shape[1]
        cat_sentence_index = max_idx // num_year
        year_sentence_index = max_idx % num_year
        best_sentence_pair[year_label] = {
            'Similarity': sim_scores[max_idx],
            'Category Sentence': cat_sent_list[cat_sentence_index],
            'Year Sentence': year_sentences[year_sentence_index]
        }

    # ---------------------------
    # 3-1. 요약 통계 출력 (DataFrame)
    # ---------------------------
    df_stats = pd.DataFrame(summary_stats).T
    df_stats.index.name = 'Year'
    print("\n[유사도 요약 통계]")
    print(df_stats)

    # ---------------------------
    # 3-2. 두 연도의 유사도 분포를 하나의 DataFrame으로 결합하여 시각화
    # ---------------------------
    df_box = pd.DataFrame({
        'Similarity': np.concatenate([sim_scores_dict['2015년'], sim_scores_dict['2022년']]),
        'Year': ['2015년'] * len(sim_scores_dict['2015년']) + ['2022년'] * len(sim_scores_dict['2022년'])
    })

    # (a) 박스플롯 비교
    plt.figure(figsize=(8, 4))
    sns.boxplot(x='Year', y='Similarity', data=df_box)
    plt.title(f"기준서 {cat_idx} - 연도별 유사도 비교 (Boxplot)")
    plt.ylim(0, 1)
    plt.show()

    # (b) 히스토그램 비교
    plt.figure(figsize=(8, 4))
    bins = np.linspace(0, 1, 21)  # 20 구간
    sns.histplot(data=df_box, x='Similarity', hue='Year', bins=bins, multiple='dodge', edgecolor='black')
    plt.title(f"기준서 {cat_idx} - 연도별 유사도 분포 (Histogram)")
    plt.xlabel("코사인 유사도")
    plt.ylabel("문장 개수")
    plt.show()

    # ---------------------------
    # 3-3. 각 연도별 가장 유사한 문장 쌍 출력
    # ---------------------------
    for year_label in ["2015년", "2022년"]:
        best = best_sentence_pair[year_label]
        print(f"\n[{year_label} - 가장 유사한 문장 쌍 (유사도 {best['Similarity']:.3f})]")
        print("  기준서 문장 :", best['Category Sentence'])
        print("  해당 연도 문장 :", best['Year Sentence'])

    # 요약 통계를 저장 (Category 정보 포함)
    df_stats['Category'] = f"기준서 {cat_idx}"
    all_summary.append(df_stats)

# ====================================
# 4. 전체 기준서 요약 통계 (모든 기준서 비교)
# ====================================
combined_summary = pd.concat(all_summary)
print("\n===== 전체 기준서별 유사도 요약 통계 =====")
print(combined_summary.reset_index())

